{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch\n",
    "================\n",
    "\n",
    "파이토치는 파이썬 기반의 scientific computing package로 아래 독자층을 타겟하고 있다\n",
    "\n",
    "-  GPU를 이용할 수 있는 NumPy 대체 패키지\n",
    "-  최고의 유연성과 속도를 제공하는 딥러닝 연구 플랫폼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치 시작하기\n",
    "---------------\n",
    "\n",
    "### 텐서(Tensors)\n",
    "\n",
    "텐서는 NumPy의 ndarray와 비슷하며 GPU를 활용한 계산을 부가적으로 제공한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서 초기화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기화되지 않은 5x3 tensor 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤하게 초기화된 텐서 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "값이 0이고 dtype이 long인 텐서 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "값이 1이고 dtype이 float16인 텐서 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "shape = (2,3,)\n",
    "ones_tensor = torch.ones(shape, dtype = torch.float16)\n",
    "\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미 존재하는 데이터로 부터 텐서 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x = torch.tensor(data)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미 존재하는 tensor를 이용하여 새로운 텐서 생성하기\n",
    "\n",
    "이 함수들은 tensor의 새로운 속성 (e.g. dtype, size)을 전달받지 않는한, input tensor의 속성을 그대로 재사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = x.new_ones(2, 3)      # override shape\n",
    "print(x)                  # result has the same dtype\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype\n",
    "print(x)                                      # result has the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy로 부터 tensor 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "np_array = np.array([[1, 2],[3, 4]])\n",
    "x = torch.from_numpy(np_array)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch Tensor와 NumPy array는 메모리 위치를 공유한다 (Torch Tensor가 CPU 위에 있는 경우)\n",
    "\n",
    "따라서 둘중 하나의 값을 변경하면 다른것의 값도 변경된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "np.add(np_array, 1, out=np_array)\n",
    "print(f\"numpy: \\n{np_array}\\n\")\n",
    "print(f\"tensor: \\n{x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor로 부터 numpy array 가져오기\n",
    "\n",
    "CPU위에 있는 모든 텐서는 Numpy로 변환거다 역변환하는것을 지원한다 (CharTensor 제외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "a = torch.ones(3, 5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서의 속성 (Attributes of tensor)\n",
    "- size : 데이터 shape\n",
    "- dtype : 개별 데이터의 자료형\n",
    "- device : 어느 device (CPU/GPU)위에 있는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.rand(3,4)\n",
    "\n",
    "print(f\"Size of tensor: {x.size()}\")\n",
    "print(f\"Datatype of tensor: {x.dtype}\")\n",
    "print(f\"Tensor is stored on device: {x.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 연산 (Operations on tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산의 위치 (Operation locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서는 ``.to`` 메서드를 이용해 다른 디바이스로 옮길 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.rand(3,4)\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = x.to(\"cuda\")\n",
    "    \n",
    "    print(f\"Shape of tensor: {x_gpu.shape}\")\n",
    "    print(f\"Datatype of tensor: {x_gpu.dtype}\")\n",
    "    print(f\"Tensor is stored on device: {x_gpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 위에 있는 텐서끼리는 연산을 할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")           # a CUDA device object\n",
    "    y_gpu = torch.ones_like(x, device=device)   # directly create a tensor on GPU\n",
    "    x_gpu = x.to(device)                        # or just use strings ``.to(\"cuda\")``\n",
    "    z = x_gpu + y_gpu\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.float16))       # .to를 이용해 dtype도 같이 변환할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서로 다른 디바이스에 있는 텐서끼리는 연산할 수 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x + y_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 산술 연산 (Arithmatic Operations)\n",
    "PyTorch에는 다양한 연산들이 미리 정의 되어 있으며, 하나의 연산에도 여러 syntax가 존재한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "덧셈: syntax 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.ones(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "덧셈: syntax 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 저장할 텐서를 argument로 전달하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in-place 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# adds x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>모든 in-place연산의 함수 이름은 ``_``로 끝난다.\n",
    "    예: ``x.copy_(y)``, ``x.t_()`` 등의 연산은 ``x``의 값을 변경할 것이다.</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스칼라 곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x_t = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "c = 10\n",
    "\n",
    "x_t = x_t * c #broadcasting\n",
    "print(x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬 곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x1 = torch.tensor([[1,2], [3,4]], dtype = torch.float32)\n",
    "x2 = torch.tensor([[1,2,3],[4,5,6]], dtype = torch.float32)\n",
    "\n",
    "y1 = torch.matmul(x1, x2) #\n",
    "y2 = x1.matmul(x2)\n",
    "y3 = x1 @ x2\n",
    "\n",
    "y4 = torch.rand_like(y1)\n",
    "torch.matmul(x1, x2, out=y4)\n",
    "\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)\n",
    "print(y4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "element-wise 곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2], [3,4]], dtype = torch.float32)\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = x * x\n",
    "z2 = x.mul(x)\n",
    "\n",
    "z3 = torch.rand_like(x)\n",
    "torch.mul(x, x, out=z3)\n",
    "\n",
    "print(z1)\n",
    "print(z2)\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "집계 함수 (aggregation functions)\n",
    "다양한 집계 함수들이 이미 정의 되어 있으며 필요할 경우 documentation을 찾아보고 이용한다\n",
    "\n",
    "집계 후의 텐서 값을 가져오기 위해서는 ``.item()``함수를 이용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2], [3,4], [5,6]], dtype = torch.float32)\n",
    "agg = x.sum(axis = 1)\n",
    "print(agg)\n",
    "\n",
    "agg_item = agg[0].item()\n",
    "print(agg_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.randn(3, 2)\n",
    "print(x)\n",
    "print(x[1,1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인덱싱과 슬라이싱 (indexing and slicing)\n",
    "Numpy에서 이용하였던 방법을 그대로 모두 이용할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
    "print(f\"{x}\")\n",
    "print(f\"Shape of tensor: {x.shape}\")\n",
    "print(f\"First row: {x[0]}\")\n",
    "print(f\"First column: {x[:, 0]}\")\n",
    "print(f\"Last column: {x[..., -1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]]) #shape : (4, 3)\n",
    "x[:,1] = 0      # scalar broadcasted to tensor of size (4, 1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.ones(2,3,4)\n",
    "sub_tensor = torch.randn(2,4)\n",
    "x[0,1:3 ,:] = sub_tensor\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.ones(2,3,4)\n",
    "sub_tensor = torch.randn(1,4)\n",
    "x[0, 1:3 ,:] = sub_tensor # tensor with (1,4) shape broadcasted to (2, 4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.arange(1, 16)\n",
    "print(x)\n",
    "y = x.view(5, 3)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>실습</mark>\n",
    "\n",
    "아래 값을 가지는 `torch.tensor` 를 생성하라\n",
    "\n",
    "$\\begin{bmatrix} 1 & 2.2 & 9.6 \\\\ 4 & -7.2 & 6.3 \\end{bmatrix}$\n",
    "\n",
    "`.mean()` 함수를 이용해 각 행과 열의 평균을 계산하라.\n",
    "\n",
    "각 결과의 shape을 출력하라\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "##### YOUR CODE START #####\n",
    "\n",
    "##### YOUR CODE END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd: Automatic Differentiation\n",
    "===================================\n",
    "\n",
    "PyTorch의 가장 중요한 패키기중 하나는 ``autograd`` 패키지 이다.\n",
    "\n",
    "``autograd`` 패키지는 Tensor의 모든 연산에 대응하는 미분을 자동으로 계산한다.\n",
    "연산 방법은 런타임에 자동으로 정의되며, 모든 backprop은 당신의 코드가 실행되면서 결정된다. 즉 매 스텝마다 미분 계산이 달라질 수 있다.\n",
    "\n",
    "``torch.Tensor`` 의 속성 중 ``.requires_grad`` 를 ``True``로 설정하게 되면 ``autograd``패키지는 이 텐서에 수행되는 모든 연산을 추적한다.\n",
    "연산이 끝난뒤에는 ``.backward()`` 함수를 호출하여 gradient를 모두 자동으로 계산할 수 있다.\n",
    "이 텐서에 해당하는 gradient는 ``.grad`` 속성에 저장되며 점차 쌓이게 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``requires_grad=True``인 tensor를 하나 생성하여 이 변수의 연산을 tracking해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor 연산을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``y``는 ``x``를 이용한 연산의 결과로 생겼으므로 ``grad_fn`` 속성을 갖고 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``y``에 다른 연산을 더 적용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backprop을 위해서는 target 값에 ``backward()``함수를 호출한다.\n",
    "\n",
    "즉 아래는 \n",
    "$\\frac{d(out)}{dx}$\n",
    "미분값을 출력한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맞는지 확인해보자\n",
    "\n",
    "$out = \\frac{1}{4}\\sum_i 3(x_i+2)^2$\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$\n",
    "\n",
    "hence, \n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Forward pass에서 autograd는 아래 두가지 일을 수행한다:\n",
    "* 결과 텐서를 계산하기 위해 연산을 수행한다.\n",
    "* 자동미분을 위해 연산에 대응하는 gradient function를 DAG (directed acyclic graph)형태로 저장한다.\n",
    "\n",
    "backward pass는 ``backward()`` 함수가 DAG root에서 호출될 경우 진행된다:\n",
    "* 각 ``.grad_fn`` 에 대한 미분을 계산한다\n",
    "* 그 결과값을 대응하는 텐서의 ``.grad`` 속성에 쌓는다 (즉 계산된 gradient는 ``.grad``속성에 저장된다)\n",
    "* 연쇄법칙(chain rule)을 통해 leaf tensor까지 전달한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.requires_grad=True``인 텐서에 대해서 autograd 추적을 원하지 않을 경우 \n",
    "\n",
    "``with torch.no_grad():`` 문에 코드를 둘러싼다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(x.requires_grad)\n",
    "z = x ** 2\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tz = x ** 2\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 ``.detach()``를 사용하여 값은 동일하지만 requires_grad=False인 새로운 텐서를 얻는다\n",
    "\n",
    "이렇게 gradient tracking을 끄고 싶은 경우는 경우는 아래와 같은 경우가 있다.\n",
    "1. Neural Network의 특정 파라미터들을 frozen 시키고 싶을 경우. 예: finetuning a pretrained network\n",
    "\n",
    "2. forward pass만 필요하기 떄문에 계산을 빨리하고 싶은 경우 (예: 모델 평가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "\n",
    "print(\"Is all the value is identical? \", x.eq(y).all().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.requires_grad_``를 사용해 텐서의 ``requires_grad``속성을 in-place로 변경ㄴ할 수도 있다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 아래와 같은 함수를 생각해보자 $$f = x^2 + y^2 + z^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "z = torch.tensor(1.5, requires_grad=True)\n",
    "f = x**2+y**2+z**2\n",
    "f.backward()\n",
    "print(f\"grads: \", x.grad, y.grad, z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두번째 ``backward()``호출은 에러를 일으킨다. 즉 하나의 computational graph에 한번의 backward만 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "f.backward() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 ``retain_graph``= True로 두면 gradient를 accumulation할수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "z = torch.tensor(1.5, requires_grad=True)\n",
    "f = x**2+y**2+z**2\n",
    "\n",
    "f.backward(retain_graph=True)\n",
    "print(f\"grads on First call: \", x.grad, y.grad, z.grad)\n",
    "f.backward(retain_graph=True)\n",
    "print(f\"grads on Second call: \", x.grad, y.grad, z.grad) # PyTorch accumulates the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Neural Network\n",
    "===============\n",
    "\n",
    "Neural networks 는 레이어/모듈로 구성되어 있으며 ``torch.nn``에 필요한 모든 building block이 정의되어 있다.\n",
    "\n",
    "모든 PyTorch 모듈은 ``nn.Module``을 상속(subclass)하며 당신이 구현하는 neural network도 그 자체로 모듈이며 다른 모듈(혹은 레어어)를 포함할 수 있다\n",
    "\n",
    "이렇게 모듈이 중첩된 구조(nested structure)로 당신의 Neural network를 구성함으로써 매우 복잡한 아키텍쳐도 쉽게 관리할 수 있다.\n",
    "\n",
    "``nn`` 패키지는 ``autograd``를 이용하여 모델 파라미터를 미분한다.\n",
    "\n",
    "신경망 학습과정은 보통 다음의 과정을 통해 이루어진다:\n",
    "- 신경망과 learnable parameter (weight)들을 정의한다 \n",
    "- 데이터를 순회하며 신경망의 forward propagation을 수행한다.\n",
    "- loss를 계산한다.\n",
    "- backward를 수행한다.\n",
    "- gradient descent에 따라 weight를 업데이트 한다:\n",
    "\n",
    "  ``weight = weight - learning_rate * gradient``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 간단한 neural network를 정의해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(100, 10) ## an linear operation: y = Wx + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``nn.Module``는 다음과 같은 요소들로 구성되어 있다.\n",
    "\n",
    "- `__init__` 함수에서는 신경망에서 사용할 모듈(module)과 레이어(layer)를 정의하며 이를 통해 학습 가능한 파라미터(learnable parameters)가 정의되고 초기화된다.\n",
    "\n",
    "- `forward` 함수는 신경망의 순전파(forward propagation) 과정에서 수행될 텐서 연산을 정의한다. PyTorch에서 제공하는 다양한 텐서 연산뿐만 아니라, Python의 기본 연산도 자유롭게 사용할 수 있다. 순전파의 계산 결과를 리턴한다.\n",
    "\n",
    "- `backward` 함수는 역전파(backpropagation)를 위해 `autograd`에 의해 자동으로 정의된다. 사용자는 `backward` 함수를 직접 구현할 필요가 없으며, PyTorch가 자동으로 기울기를 계산하고 가중치를 업데이트한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "net = LinearNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.parameters()`` 메서드를 통해 learnable parameter를 얻을 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) # Matrix W\n",
    "print(params[1].size()) # bias b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "1x100 크기의 random input을 전달해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 100)\n",
    "output = net(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고: ``torch.nn``는 mini-batch만 지원한다.\n",
    "\n",
    "예를들어 ``nn.Linear`` 레이어는 (batch_size, input_size)의 2차원 텐서를 입력받는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "loss function은 입력 데이터에 대응하는 모델의 출력과, target(label) 쌍을 입력받아 출력과 target의 차이를 계산함으로써 모델이 얼마나 틀린 결과를 내고 있는지에 측정값을 제공한다. 우리는 학습과정에서 loss를 최소화함으로써 모델의 성능을 개선할 수 있다.\n",
    "\n",
    "자주 사용되는 loss 함수는 회귀를 위한 nn.MSELoss (Mean Square Error)와 분류를 위한 nn.NLLLoss (Negative Log Likelihood) 등이 있으며, [PyTorch documentation](<https://pytorch.org/docs/stable/nn.html#loss-functions>)을 참고하기 바람."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target\n",
    "target = target.view(1, -1)  # make the shape same with output (1, 10)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 역전파 수행을 위해 ``loss.backward()``를 호출할 수 있다.\n",
    "\n",
    "loss 계산을 위한 사용된 모든 computational graph에 대하여 미분을 수행하며,\n",
    "graph상의 tensor들 중 ``requires_grad=True``인 것들은 ``.grad``에 계산된 gradient가 축적된다.\n",
    "\n",
    "즉, $\\frac{\\partial L}{\\partial W}$ 가 계산된다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어, 몇 단계의 backward를 따라가 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop\n",
    "``loss.backward()``를 이용하여 Backpropagation을 수행하자.\n",
    "\n",
    "backward 수행 후 parameter의 gradient가 계산된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(f'linear.bias.grad before backward : {net.linear.bias.grad}')\n",
    "loss.backward()\n",
    "print(f'linear.bias.grad after backward {net.linear.bias.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization (Update the weights)\n",
    "Optimization은 학습과정에서 model parameters를 조정하여 모델의 에러를 줄이는 과정이다. \n",
    "\n",
    "Optimization 알고리즘은 어떠한 방식으로 모델 파라미터를 업데이트할지를 정의한다.\n",
    "\n",
    "가장 간단한 파라미터 업데이트 방법은 Stochastic Gradient Descent (SGD)이다\n",
    "\n",
    "```\n",
    "weight = weight - learning_rate * gradient\n",
    "```\n",
    "\n",
    "SGD의 파라미터 업데이트는 다음과 같이 수행할 수 있다.\n",
    "``` python\n",
    "learning_rate = 0.01\n",
    "for p in net.parameters():\n",
    "    p.data.sub_(p.grad.data * learning_rate)\n",
    "```\n",
    "\n",
    "하지만 우리는 이 방법 외에도 매우 다양한 방법으로 파라미터를 업데이트 하고자 하며, Adam, RMSProc을 포함하는 다양한 optimization 방법들이 ``torch.optim``에 구현되어 있다.\n",
    "\n",
    "따라서 우리는 optimizer라는 별도의 객체에 파라미터를 업데이트를 위임하며,\n",
    "optimizer를 초기화할 때 모델 파라미터를 전달하여 optimizer가 학습과정에서 파라미터를 대신 업데이트할 수 있도록 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01) # Neural network의 파라미터들을 입력으로 받는다.\n",
    "\n",
    "print(f'linear.bias before step : {net.linear.bias}\\n')\n",
    "\n",
    "# A single training step consist of:\n",
    "optimizer.zero_grad()   # 이전 스텝이서 계산된 gradient를 지운다.\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Optimerzer가 Parameter update를 대신 해준다.\n",
    "\n",
    "print(f'linear.bias after step : {net.linear.bias}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero_grad()가 필요한 이유는 한 computational graph에 한번의 back_prop만이 가능하기 때문이다. \n",
    "\n",
    "(혹은 ``retain_graph`` = True일 경우 gradient가 쌓임)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축하합니다! 당신은 지금까지 neural networks를 정의하고, loss를 계산하고, weights를 업데이트 하는 방법에 대해 모두 배웠습니다.\n",
    "이제는 당신만의 신경망을 학습할 준비가 되었습니다.\n",
    "\n",
    "\n",
    "**참고:**\n",
    "  -  ``torch.Tensor`` - *multi-dimensional array*로서 autograd와 GPU연산을 지원함. gradient값도 보관한다.\n",
    "  -  ``nn.Module`` - Neural network 모듈. 모델 파라미터를 은닉(encapsulate)하고 편리하게 관리할 수 있도록 해주며, GPU로의 이동, export, loading등 다양한 편의성도 제공한다.\n",
    "  -  ``nn.Parameter`` - 텐서의 일종으로 nn.Module의 속성에 할당될 경우 자동으로 파라미터로 등록됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Multi-layer perceptron\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터셋 가져오기\n",
    "학습의 시작은 데이터를 읽어오는 것으로, 먼저 원하는 데이터를 읽어와 torch.tensor로 변환을 수행한다.\n",
    "\n",
    "아래와 같은 패키지들이 유용하다.\n",
    "\n",
    "-  For images, packages such as Pillow, OpenCV \n",
    "-  For audio, packages such as scipy and librosa\n",
    "-  For text, either raw Python or Cython based loading, or NLTK and\n",
    "   SpaCy\n",
    "\n",
    "\n",
    "``torchvision`` 패키지에는 vision분야에서 자주 사용되는 몇몇 유명한 데이터셋을 읽어오는 함수를 제공한다.\n",
    "\n",
    "우리는 이번에 MNIST데이터셋을 분류하는 Multi-layter Perceptron모델을 구현해볼 것이다.\n",
    "\n",
    "MNIST 데이터셋은 handwritten digits 분류하는, 머신러닝 분야에서 매우 유명한 데이터셋이다.\n",
    "\n",
    "이미지 분류모델을 아래 과정을 통해 학습된다.\n",
    "\n",
    "1. ``torchvision``을 이용하여 데이터셋을 읽어온다.\n",
    "2. Neural Network를 정의한다\n",
    "3. loss 함수를 정의한다.\n",
    "4. training data를 이용하여 학습한다.\n",
    "5. test data를 이용하여 모델을 평가한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리를 하는 코드들은 지저분하기 마련이며 관리하기도 어렵다.\n",
    "\n",
    "이상적으로는 데이터셋을 관리하는 코드와 모델을 학습하는 코드가 완전히 분리되는것이 좋다 (for better readability, resuability, and modularity).\n",
    "\n",
    "PyTorch에서는 데이터를 관리하기 위한 두가지 모듈을 제공한다: ``torch.utils.data.Dataset``, ``torch.utils.data.DataLoader``\n",
    "\n",
    "``Dataset``는 샘플 데이터와 라벨을 저장하고 있으며, iterator가 감싸져있어 쉽게 데이터에 접근할 수 있다.\n",
    "\n",
    "``torchvision.datasets``는 ``torch.utils.data.Dataset``를 상속하는 다양한 데이터셋을 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST_datasets(data_root_dir):\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root=data_root_dir, train=True, download=True, \n",
    "        transform=ToTensor() # convert PILImage images of range [0, 1] to tensors of normalized range [-1, 1].\n",
    "    )\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root=data_root_dir, train=False, download=True, \n",
    "        transform=ToTensor()\n",
    "    )\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_MNIST_datasets(\"/datasets\")\n",
    "\n",
    "print(\"Train size: \", len(train_dataset))\n",
    "print(\"Test size: \", len(test_dataset))\n",
    "print(\"Image size: \", train_dataset[0][0].shape)\n",
    "print(\"Label of fisrt example: \", train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_few_samples(dataset, cols=8, rows=5):\n",
    "    figure = plt.figure(figsize=(6, 4))\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        sample_idx = torch.randint(len(dataset), size=(1,)).item()\n",
    "        img, label = dataset[sample_idx]\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        plt.title(label)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "visualize_few_samples(train_dataset, cols = 8, rows = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Dataset``은 데이터 feature와 라벨을 한번에 하나씩 가져오는 기능을 제공한다. 하지만 보통 학습에서는 샘플들을 “minibatches”로 가져온고, 매 epoch마다 랜덤하게 섞어주며, multiprocessing을 사용해 데이터 획득을 빠르게 하고자 한다.\n",
    "\n",
    "DataLoader이 복잡한 과정을 쉽도록 도와주는 iterable이다.\n",
    "\n",
    "- 데이터로더는 데이터셋을 배치 단위로 묶어준다.\n",
    "- ``shuffle``를 통해 매 epoch마다 랜덤하게 섞어주는 기능을 제공한다\n",
    "- ``num_workers``를 통해 데이터 전처리를 multiprocessing으로 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Network 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch documentation](https://pytorch.org/docs/stable/nn.html)을 참고하여 3개의 레이어로 Multi-layer Perceptron모듈을 작성하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        ##### YOUR CODE START #####  \n",
    "        # 1. First Layer\n",
    "        #    - Linear layer with input size `in_dim` and output size `hidden_dim`\n",
    "        #    - ReLU (Rectified Linear Unit) activation\n",
    "        # 2. Second Layer:\n",
    "        #    - Linear layer with input size `hidden_dim` and output size `hidden_dim`\n",
    "        #    - ReLU (Rectified Linear Unit) activation\n",
    "        # 3. Output Layer:\n",
    "        #    - Linear layer with input size `hidden_dim` and output size `out_dim`\n",
    "        #    - `out_dim` units represent the number of classes in a classification task\n",
    "        # Use nn.Sequential() to stack these layers together.\n",
    "             \n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "        # Write a forward pass of your model\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model을 사용하기 위해 input data를 () 연산을 통해 전달하면, 이를 통해 ``forward`` 함수와 더불어 여러 작업들이 자동으로 수행된다.\n",
    "\n",
    "직접 ``model.forward()``를 호출하지 말것!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# test if your model works with your input data\n",
    "model = MultiLayerPerceptron(in_dim = 28*28, hidden_dim = 512, out_dim = 10)\n",
    "print(model)\n",
    "\n",
    "X = torch.rand(16, 1, 28, 28) # dummy data for testing with batch_size 16\n",
    "logits = model(X) \n",
    "pred_prob = nn.Softmax(dim=1)(logits) # Predicted probability for each class with shape (batch_size, 10)\n",
    "print(pred_prob.shape)\n",
    "y_pred = pred_prob.argmax(axis = 1)\n",
    "print(f\"Predicted classes: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리 모델은 아래와 같이 구성된다:\n",
    "- nn.Flatten: 이차원 28x28 이미지를 연속된 784 픽셀값을 가지는 1차원 텐서로 변환한다 (minibatch dimension (at dim=0)은 그대로 유지됨).\n",
    "- nn.Linear: linear레이어는 입력에 대하여 learnable weights와 biases를 이용하여 선형 변환을 수행하는 모듈이다 (Fully connected layer)\n",
    "- nn.ReLU: Non-linear activations. 선형 변환 후에 적용하여 비선형성을 추가하며, 모델이 다양한 분포를 학습할수 있도록 해준다.\n",
    "- nn.Sequential: 순서가 있는 모듈의 컨테이너(container). 데이터는 컨테이너 내의 모든 모듈들을 정의된것과 같은 순서로 통과한다.\n",
    "- nn.Softmax: MLP의 마지막 레이어는 [-infty, infty]에 분포하는 logit 값을 리턴한다. logit값은 nn.Softmax모듈에 전달되거 [0, 1]사이의 확률값으로 변환된다. dim 인자를 통해 어느 축의 값이 더해서 1이 될지 정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정의한 모델은 그 내부에 parameter(weights and biases)를 갖고 있다.\n",
    "\n",
    "Network를 정의할때 ``nn.Module``를 상속하게 되면 모델에 정의된 모든 파라미터를 자동으로 추적하며 ``parameters()`` 또는 ``named_parameters()`` 메서드를 통해 접근 가능해지게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}\\t| Size: {param.size()}\\t| Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizing your model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 통해 backward()를 수행시 모델 파라미터의 gradient가 계산되어 저장되는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron(in_dim = 28*28, hidden_dim = 512, out_dim = 10)\n",
    "\n",
    "X = torch.rand(16, 1, 28, 28) # dummy data with batch_size 16\n",
    "logits = model(X) # forward pass return a logits with shape (batch_size, 10)\n",
    "\n",
    "print(\"----- Parameters before backward() -----\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer {name}\\t| Size: {param.size()}\\t| grad : {param.grad}\")\n",
    "\n",
    "\n",
    "y = torch.randint(10, (16,))\n",
    "loss = nn.CrossEntropyLoss()(logits, y)\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n----- Parameters after backward() -----\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer {name}\\t| Size: {param.size()}\\t| grad : {param.grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 parameter 업데이트를 통해 학습을 수행하자.\n",
    "\n",
    "학습은 다음의 과정으로 구성된다.\n",
    "\n",
    "for  each iteration (called an **epoch**):\n",
    "- forward pass를 통하여 output (guess) 계산\n",
    "- output과 target의 차이를 통해 error(loss)를 계산\n",
    "- 파라미터에 대한 loss의 값의 미분 계산\n",
    "- 경사하강법(gradient descent)를 통하여 파라미터 optimize\n",
    "\n",
    "각 epoch은 다음의 과정으로 구성된다\n",
    "- 학습 루프(Train Loop) - 학습 데이터를 수행하여 최적 파라미터를 학습한다\n",
    "- Evaluation(Validation/Test) Loop - 평가 데이터셋을 순회하며 모델의 성능이 좋아지고 있는지 평가한다.\n",
    "\n",
    "아직 이 과정이 익숙하지 않은 학생들은 [영상](https://www.youtube.com/watch?v=tIeHLnjs5U8)을 참고할것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>과제</mark>\n",
    "``train_loop``와 ``evaluation_loop``를 완성하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, device, dataloader, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() #Switch to train mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        ##### YOUR CODE START #####\n",
    "        # Forward propagation and compute loss\n",
    "\n",
    "        # Backpropagation and update parameter\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "    \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 300 == 0:\n",
    "            print(f\"Train loss: {loss.item():>7f}  [{(batch_idx + 1) * len(X):>5d}/{size:>5d}]\")\n",
    "\n",
    "    avg_train_loss = running_loss / len(dataloader)\n",
    "    return(avg_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(model, device, dataloader, loss_fn):    \n",
    "    model.eval() # Set the model to evaluation mode. important for batch normalization and dropout layers\n",
    "\n",
    "    test_loss, correct = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            ##### YOUR CODE START #####\n",
    "            # accumulate test_loss : sum of loss over mini-batches\n",
    "            # accumulate correct : the number of correct prediction (hint: use argmax)\n",
    "\n",
    "            ##### YOUR CODE END #####\n",
    "\n",
    "    avg_test_loss = test_loss / len(dataloader)\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    print(f\"\\nTest Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {avg_test_loss:>8f} \\n\")\n",
    "\n",
    "    return(avg_test_loss, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 학습 수행\n",
    "\n",
    "실제 학습 수행을 위해서는 hyperparameter를 지정해주어야 한다.\n",
    "하이퍼 파라미터는 모델 학습 과정을 컨트롤 할수 있는 파라미터로, 모델 학습 및 수렴속도를 결정짓는 중요한 값들입니다.\n",
    "\n",
    "- Number of Epochs: 데이터 전체를 반복할 횟수\n",
    "- Batch Size: mini-batch 사이즈. \n",
    "- Learning Rate: 각 iteration마다 얼마나 모델 파라미터를 업데이트 할지.\n",
    "\n",
    "``nn.CrossEntropyLoss``는 ``nn.LogSoftmax``와 ``nn.NLLLoss``를 합친 loss function이다\n",
    "\n",
    "현재까지 배운것을 모두 합쳐 학습을 수행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hyper parameters\n",
    "    batch_size = 64\n",
    "    learning_rate = 1e-3\n",
    "    epochs = 40\n",
    "\n",
    "    # training setting\n",
    "    in_dim, hidden_dim, out_dim = 28*28, 512, 10\n",
    "    save_model = True\n",
    "    \n",
    "    seed = 1\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "    train_dataset, test_dataset = load_MNIST_datasets(\"/datasets\")\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset= train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(dataset= test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    model = MultiLayerPerceptron(in_dim, hidden_dim, out_dim).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    train_loss_list, test_loss_list, test_accuracy_list = [], [], []\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loss = train_loop(model, device, train_dataloader, loss_fn, optimizer)\n",
    "        test_loss, test_accuracy = evaluation_loop(model, device, test_dataloader, loss_fn)\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "        print(\"Saved Model State to model.pth\")\n",
    "\n",
    "    return train_loss_list, test_loss_list, test_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_loss_list, test_loss_list, test_accuracy_list = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "epochs_range = [i for i in range(1, len(train_loss_list) + 1)]\n",
    "plt.plot(epochs_range, train_loss_list, label='Train Loss')\n",
    "plt.plot(epochs_range, test_loss_list, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "epochs_range = [i for i in range(1, len(train_loss_list) + 1)]\n",
    "plt.plot(epochs_range, test_accuracy_list, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron(28*28, 512, 10)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(test_dataloader))\n",
    "    logits = model(X)\n",
    "    y_pred = logits.argmax(dim = 1)\n",
    "\n",
    "    print(f'Predicted: \"{y_pred}\"')\n",
    "    print(f\"Actual: {y}\")\n",
    "\n",
    "    # display single example\n",
    "    img = X[0].squeeze()\n",
    "    label = y[0]\n",
    "    pred_label = y_pred[0]\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Predicted label: {pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
