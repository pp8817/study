{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import wandb\n",
    "\n",
    "from training_utilities import train_loop, evaluation_loop, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습시간에는 다양한 학습 전략과 hyperparameter tuning을 통해 CIFAR-10 테스트셋에서 높은 분류 성능을 얻는 것이 목표이다.\n",
    "\n",
    "<mark>과제</mark> 다양한 조건에서 CIFAR-10 데이터셋 학습을 실험해보고 test 데이터셋에서 80% 이상의 accuracy를 달성하라.\n",
    "\n",
    "* 제출물1 : <u>5개 이상의 학습 커브</u>를 포함하는 wandb 화면 캡처 (wandb 웹페이지의 본인 이름 포함하여 캡처)\n",
    "* 제출물2 : 실험 결과에 대한 분석과 논의 (아래에 markdown으로 기입)\n",
    "\n",
    "참고: 코드에 대한 pytest가 따로 없으므로 자유롭게 코드를 변경하여도 무방함.\n",
    "\n",
    "단, <U>Transfer learning 혹은 Batch size는 변경은 수행하지 말것</U>\n",
    "\n",
    "실험 조건 예시\n",
    "- [Network architectures](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n",
    "- input normalization\n",
    "- [Weight initialization](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_)\n",
    "- [Optimizers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) (Adam, SGD with momentum, ... )\n",
    "- Regularizations (weight decay, dropout, [Data augmentation](https://pytorch.org/vision/0.9/transforms.html), ensembles, ...)\n",
    "- learning rate & [learning rate scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n",
    "\n",
    "스스로 neural network를 구축할 경우 아래 사항들을 고려하라\n",
    "- Filter size\n",
    "- Number of filters\n",
    "- Pooling vs Strided Convolution\n",
    "- Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, num_classes, config):\n",
    "    if model_name == \"resnet50\":\n",
    "        model = models.resnet50()\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    elif model_name == \"vgg16\":\n",
    "        model = models.vgg16()\n",
    "        model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(25088, 4096),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Model not supported: {}\".format(model_name))\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Using model {model_name} with {total_params} parameters ({trainable_params} trainable)\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_dataloaders(data_root_dir, device, batch_size, num_worker):\n",
    "    validation_size = 0.2\n",
    "    random_seed = 42\n",
    "\n",
    "    normalize = transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5)) \n",
    "    \n",
    "    # train_transforms = transforms.Compose([\n",
    "    #     transforms.ToTensor(),\n",
    "    #     normalize,\n",
    "    # ])\n",
    "\n",
    "    # 데이터 전처리\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=train_transforms)\n",
    "    val_dataset = datasets.CIFAR10(root=data_root_dir, train=True, download=True, transform=test_transforms)\n",
    "    test_dataset = datasets.CIFAR10(root=data_root_dir, train=False, download=True, transform=test_transforms)\n",
    "\n",
    "    num_classes = len(train_dataset.classes)\n",
    "\n",
    "    # Split train dataset into train and validataion dataset\n",
    "    train_indices, val_indices = train_test_split(np.arange(len(train_dataset)), \n",
    "                                                  test_size=validation_size, random_state=random_seed)\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # DataLoader\n",
    "    kwargs = {}\n",
    "    if device.startswith(\"cuda\"):\n",
    "        kwargs.update({\n",
    "            'pin_memory': True,\n",
    "        })\n",
    "\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                  num_workers=num_worker, **kwargs)\n",
    "    val_dataloader = DataLoader(dataset = val_dataset, batch_size=batch_size, sampler=valid_sampler,\n",
    "                                num_workers=num_worker, **kwargs)\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                                 num_workers=num_worker, **kwargs)\n",
    "    \n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')  # 변경된 부분\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')  # 변경된 부분\n",
    "\n",
    "def train_main(config):\n",
    "    ## data and preprocessing settings\n",
    "    data_root_dir = config['data_root_dir']\n",
    "    num_worker = config.get('num_worker', 4)\n",
    "\n",
    "    ## Hyper parameters\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    ## checkpoint setting\n",
    "    checkpoint_save_interval = config.get('checkpoint_save_interval', 10)\n",
    "    checkpoint_path = config.get('checkpoint_path', \"checkpoints/checkpoint.pth\")\n",
    "    best_model_path = config.get('best_model_path', \"checkpoints/best_model.pth\")\n",
    "    load_from_checkpoint = config.get('load_from_checkpoint', None)\n",
    "\n",
    "    ## variables\n",
    "    best_acc1 = 0\n",
    "\n",
    "    wandb.init(\n",
    "        project=config[\"wandb_project_name\"],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader, num_classes = load_cifar10_dataloaders(\n",
    "        data_root_dir, device, batch_size = batch_size, num_worker = num_worker)\n",
    "    \n",
    "    model = get_model(model_name = config[\"model_name\"], num_classes= num_classes, config = config).to(device)\n",
    "    initialize_weights(model)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) # optimizer, 변경 가능 부분, 1\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4) # 2\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4) # 3\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)  # lr, 변경 가능 부분\n",
    "\n",
    "    if load_from_checkpoint:\n",
    "        load_checkpoint_path = (best_model_path if load_from_checkpoint == \"best\" else checkpoint_path)\n",
    "        start_epoch, best_acc1 = load_checkpoint(load_checkpoint_path, model, optimizer, scheduler, device)\n",
    "\n",
    "    if config.get('test_mode', False):\n",
    "        # Only evaluate on the test dataset\n",
    "        print(\"Running test evaluation...\")\n",
    "        test_acc = evaluation_loop(model, device, test_dataloader, criterion, phase = \"test\")\n",
    "        print(f\"Test Accuracy: {test_acc}\")\n",
    "        \n",
    "    else:\n",
    "        # Train and validate using train/val datasets\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            train_loop(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "            val_acc1 = evaluation_loop(model, device, val_dataloader, criterion, epoch = epoch, phase = \"validation\")\n",
    "            scheduler.step()\n",
    "\n",
    "            if (epoch + 1) % checkpoint_save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "                is_best = val_acc1 > best_acc1\n",
    "                best_acc1 = max(val_acc1, best_acc1)\n",
    "                save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, best_acc1, is_best, best_model_path)\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msangmin881717\u001b[0m (\u001b[33msangmin881717-university-of-suwon\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/deep_learning_lab/lab_05/wandb/run-20241010_080337-5lkoybmk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning/runs/5lkoybmk' target=\"_blank\">fanciful-feather-20</a></strong> to <a href='https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning/runs/5lkoybmk' target=\"_blank\">https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning/runs/5lkoybmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using model vgg16 with 134301514 parameters (134301514 trainable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 625/625 [00:09<00:00, 68.30it/s, avg_metrics=Loss: 2.1770e+00 (n=40000)), Acc@1:  17.09 (n=40000)), Data_Time:  0.001 (n=625)), Batch_Time:  0.005 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 340.51it/s, avg_metrics=Loss: 1.9074e+00 (n=10000)), Acc@1:  28.95 (n=10000))]\n",
      "Training Epoch 2: 100%|██████████| 625/625 [00:08<00:00, 75.28it/s, avg_metrics=Loss: 1.7834e+00 (n=40000)), Acc@1:  31.93 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.83it/s, avg_metrics=Loss: 1.5749e+00 (n=10000)), Acc@1:  40.66 (n=10000))]\n",
      "Training Epoch 3: 100%|██████████| 625/625 [00:08<00:00, 74.84it/s, avg_metrics=Loss: 1.5926e+00 (n=40000)), Acc@1:  39.86 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.57it/s, avg_metrics=Loss: 1.4623e+00 (n=10000)), Acc@1:  45.93 (n=10000))]\n",
      "Training Epoch 4: 100%|██████████| 625/625 [00:08<00:00, 75.03it/s, avg_metrics=Loss: 1.4465e+00 (n=40000)), Acc@1:  46.11 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 353.17it/s, avg_metrics=Loss: 1.3227e+00 (n=10000)), Acc@1:  51.97 (n=10000))]\n",
      "Training Epoch 5: 100%|██████████| 625/625 [00:08<00:00, 75.30it/s, avg_metrics=Loss: 1.3231e+00 (n=40000)), Acc@1:  51.33 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 348.80it/s, avg_metrics=Loss: 1.1902e+00 (n=10000)), Acc@1:  55.87 (n=10000))]\n",
      "Training Epoch 6: 100%|██████████| 625/625 [00:08<00:00, 74.74it/s, avg_metrics=Loss: 1.2164e+00 (n=40000)), Acc@1:  55.84 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 348.65it/s, avg_metrics=Loss: 1.1324e+00 (n=10000)), Acc@1:  59.23 (n=10000))]\n",
      "Training Epoch 7: 100%|██████████| 625/625 [00:08<00:00, 74.82it/s, avg_metrics=Loss: 1.1259e+00 (n=40000)), Acc@1:  59.26 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.72it/s, avg_metrics=Loss: 9.9137e-01 (n=10000)), Acc@1:  63.72 (n=10000))]\n",
      "Training Epoch 8: 100%|██████████| 625/625 [00:08<00:00, 75.03it/s, avg_metrics=Loss: 1.0519e+00 (n=40000)), Acc@1:  62.22 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 344.93it/s, avg_metrics=Loss: 9.4565e-01 (n=10000)), Acc@1:  66.06 (n=10000))]\n",
      "Training Epoch 9: 100%|██████████| 625/625 [00:08<00:00, 74.32it/s, avg_metrics=Loss: 9.7740e-01 (n=40000)), Acc@1:  64.93 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 348.02it/s, avg_metrics=Loss: 9.1146e-01 (n=10000)), Acc@1:  68.12 (n=10000))]\n",
      "Training Epoch 10: 100%|██████████| 625/625 [00:08<00:00, 75.35it/s, avg_metrics=Loss: 9.0683e-01 (n=40000)), Acc@1:  67.78 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.61it/s, avg_metrics=Loss: 8.0481e-01 (n=10000)), Acc@1:  71.20 (n=10000))]\n",
      "Training Epoch 11: 100%|██████████| 625/625 [00:08<00:00, 74.08it/s, avg_metrics=Loss: 8.5778e-01 (n=40000)), Acc@1:  69.57 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 355.44it/s, avg_metrics=Loss: 7.5174e-01 (n=10000)), Acc@1:  72.92 (n=10000))]\n",
      "Training Epoch 12: 100%|██████████| 625/625 [00:08<00:00, 75.20it/s, avg_metrics=Loss: 8.0917e-01 (n=40000)), Acc@1:  71.69 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 350.89it/s, avg_metrics=Loss: 7.6019e-01 (n=10000)), Acc@1:  73.32 (n=10000))]\n",
      "Training Epoch 13: 100%|██████████| 625/625 [00:08<00:00, 74.93it/s, avg_metrics=Loss: 7.7057e-01 (n=40000)), Acc@1:  72.92 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 350.24it/s, avg_metrics=Loss: 7.9431e-01 (n=10000)), Acc@1:  72.82 (n=10000))]\n",
      "Training Epoch 14: 100%|██████████| 625/625 [00:08<00:00, 74.21it/s, avg_metrics=Loss: 7.3158e-01 (n=40000)), Acc@1:  74.46 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 351.63it/s, avg_metrics=Loss: 6.8223e-01 (n=10000)), Acc@1:  76.06 (n=10000))]\n",
      "Training Epoch 15: 100%|██████████| 625/625 [00:08<00:00, 75.11it/s, avg_metrics=Loss: 7.0170e-01 (n=40000)), Acc@1:  75.46 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 352.87it/s, avg_metrics=Loss: 6.6312e-01 (n=10000)), Acc@1:  77.17 (n=10000))]\n",
      "Training Epoch 16: 100%|██████████| 625/625 [00:08<00:00, 75.24it/s, avg_metrics=Loss: 6.7894e-01 (n=40000)), Acc@1:  76.28 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.07it/s, avg_metrics=Loss: 6.4452e-01 (n=10000)), Acc@1:  77.25 (n=10000))]\n",
      "Training Epoch 17: 100%|██████████| 625/625 [00:08<00:00, 74.31it/s, avg_metrics=Loss: 6.4615e-01 (n=40000)), Acc@1:  77.39 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.97it/s, avg_metrics=Loss: 6.4357e-01 (n=10000)), Acc@1:  78.18 (n=10000))]\n",
      "Training Epoch 18: 100%|██████████| 625/625 [00:08<00:00, 74.98it/s, avg_metrics=Loss: 6.2797e-01 (n=40000)), Acc@1:  78.27 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 353.97it/s, avg_metrics=Loss: 6.2197e-01 (n=10000)), Acc@1:  78.46 (n=10000))]\n",
      "Training Epoch 19: 100%|██████████| 625/625 [00:08<00:00, 75.49it/s, avg_metrics=Loss: 6.0149e-01 (n=40000)), Acc@1:  78.98 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 351.21it/s, avg_metrics=Loss: 5.8966e-01 (n=10000)), Acc@1:  80.13 (n=10000))]\n",
      "Training Epoch 20: 100%|██████████| 625/625 [00:08<00:00, 74.84it/s, avg_metrics=Loss: 5.7947e-01 (n=40000)), Acc@1:  79.77 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.27it/s, avg_metrics=Loss: 6.8567e-01 (n=10000)), Acc@1:  77.64 (n=10000))]\n",
      "Training Epoch 21: 100%|██████████| 625/625 [00:08<00:00, 75.10it/s, avg_metrics=Loss: 5.6425e-01 (n=40000)), Acc@1:  80.48 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 354.73it/s, avg_metrics=Loss: 5.8566e-01 (n=10000)), Acc@1:  80.56 (n=10000))]\n",
      "Training Epoch 22: 100%|██████████| 625/625 [00:08<00:00, 75.18it/s, avg_metrics=Loss: 5.4583e-01 (n=40000)), Acc@1:  80.95 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 348.07it/s, avg_metrics=Loss: 5.6939e-01 (n=10000)), Acc@1:  80.33 (n=10000))]\n",
      "Training Epoch 23: 100%|██████████| 625/625 [00:08<00:00, 74.51it/s, avg_metrics=Loss: 5.2408e-01 (n=40000)), Acc@1:  81.71 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.61it/s, avg_metrics=Loss: 5.4797e-01 (n=10000)), Acc@1:  81.37 (n=10000))]\n",
      "Training Epoch 24: 100%|██████████| 625/625 [00:08<00:00, 75.41it/s, avg_metrics=Loss: 5.0511e-01 (n=40000)), Acc@1:  82.49 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 346.72it/s, avg_metrics=Loss: 5.5325e-01 (n=10000)), Acc@1:  81.20 (n=10000))]\n",
      "Training Epoch 25: 100%|██████████| 625/625 [00:08<00:00, 75.19it/s, avg_metrics=Loss: 4.9388e-01 (n=40000)), Acc@1:  82.78 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 352.07it/s, avg_metrics=Loss: 5.4382e-01 (n=10000)), Acc@1:  81.98 (n=10000))]\n",
      "Training Epoch 26: 100%|██████████| 625/625 [00:08<00:00, 74.00it/s, avg_metrics=Loss: 4.7217e-01 (n=40000)), Acc@1:  83.66 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 348.74it/s, avg_metrics=Loss: 5.5819e-01 (n=10000)), Acc@1:  81.74 (n=10000))]\n",
      "Training Epoch 27: 100%|██████████| 625/625 [00:08<00:00, 75.21it/s, avg_metrics=Loss: 4.5779e-01 (n=40000)), Acc@1:  84.12 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 346.71it/s, avg_metrics=Loss: 5.3051e-01 (n=10000)), Acc@1:  81.89 (n=10000))]\n",
      "Training Epoch 28: 100%|██████████| 625/625 [00:08<00:00, 74.42it/s, avg_metrics=Loss: 4.5452e-01 (n=40000)), Acc@1:  84.34 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.36it/s, avg_metrics=Loss: 5.2405e-01 (n=10000)), Acc@1:  82.85 (n=10000))]\n",
      "Training Epoch 29: 100%|██████████| 625/625 [00:08<00:00, 75.23it/s, avg_metrics=Loss: 4.3235e-01 (n=40000)), Acc@1:  84.99 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 345.40it/s, avg_metrics=Loss: 5.3644e-01 (n=10000)), Acc@1:  82.25 (n=10000))]\n",
      "Training Epoch 30: 100%|██████████| 625/625 [00:08<00:00, 74.69it/s, avg_metrics=Loss: 4.2527e-01 (n=40000)), Acc@1:  85.33 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.97it/s, avg_metrics=Loss: 5.1977e-01 (n=10000)), Acc@1:  82.78 (n=10000))]\n",
      "Training Epoch 31: 100%|██████████| 625/625 [00:08<00:00, 74.72it/s, avg_metrics=Loss: 4.0544e-01 (n=40000)), Acc@1:  86.02 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 350.68it/s, avg_metrics=Loss: 5.3685e-01 (n=10000)), Acc@1:  82.77 (n=10000))]\n",
      "Training Epoch 32: 100%|██████████| 625/625 [00:08<00:00, 75.40it/s, avg_metrics=Loss: 3.8973e-01 (n=40000)), Acc@1:  86.48 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 351.11it/s, avg_metrics=Loss: 5.0536e-01 (n=10000)), Acc@1:  83.49 (n=10000))]\n",
      "Training Epoch 33: 100%|██████████| 625/625 [00:08<00:00, 75.50it/s, avg_metrics=Loss: 3.8564e-01 (n=40000)), Acc@1:  86.76 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.65it/s, avg_metrics=Loss: 5.3083e-01 (n=10000)), Acc@1:  82.69 (n=10000))]\n",
      "Training Epoch 34: 100%|██████████| 625/625 [00:08<00:00, 74.14it/s, avg_metrics=Loss: 3.7685e-01 (n=40000)), Acc@1:  87.08 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 346.82it/s, avg_metrics=Loss: 5.2426e-01 (n=10000)), Acc@1:  82.92 (n=10000))]\n",
      "Training Epoch 35: 100%|██████████| 625/625 [00:08<00:00, 75.48it/s, avg_metrics=Loss: 3.6128e-01 (n=40000)), Acc@1:  87.39 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 346.95it/s, avg_metrics=Loss: 5.0771e-01 (n=10000)), Acc@1:  83.87 (n=10000))]\n",
      "Training Epoch 36: 100%|██████████| 625/625 [00:08<00:00, 74.96it/s, avg_metrics=Loss: 3.5693e-01 (n=40000)), Acc@1:  87.55 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 343.35it/s, avg_metrics=Loss: 5.5083e-01 (n=10000)), Acc@1:  83.07 (n=10000))]\n",
      "Training Epoch 37: 100%|██████████| 625/625 [00:08<00:00, 74.17it/s, avg_metrics=Loss: 3.3893e-01 (n=40000)), Acc@1:  88.17 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.16it/s, avg_metrics=Loss: 5.0602e-01 (n=10000)), Acc@1:  84.07 (n=10000))]\n",
      "Training Epoch 38: 100%|██████████| 625/625 [00:08<00:00, 75.04it/s, avg_metrics=Loss: 3.2945e-01 (n=40000)), Acc@1:  88.43 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 343.48it/s, avg_metrics=Loss: 5.1983e-01 (n=10000)), Acc@1:  83.74 (n=10000))]\n",
      "Training Epoch 39: 100%|██████████| 625/625 [00:08<00:00, 74.13it/s, avg_metrics=Loss: 3.2241e-01 (n=40000)), Acc@1:  88.82 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 343.70it/s, avg_metrics=Loss: 5.0592e-01 (n=10000)), Acc@1:  83.97 (n=10000))]\n",
      "Training Epoch 40: 100%|██████████| 625/625 [00:08<00:00, 75.33it/s, avg_metrics=Loss: 3.2015e-01 (n=40000)), Acc@1:  88.85 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 343.21it/s, avg_metrics=Loss: 4.9496e-01 (n=10000)), Acc@1:  84.22 (n=10000))]\n",
      "Training Epoch 41: 100%|██████████| 625/625 [00:08<00:00, 75.44it/s, avg_metrics=Loss: 3.0561e-01 (n=40000)), Acc@1:  89.37 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.71it/s, avg_metrics=Loss: 5.1600e-01 (n=10000)), Acc@1:  83.83 (n=10000))]\n",
      "Training Epoch 42: 100%|██████████| 625/625 [00:08<00:00, 74.39it/s, avg_metrics=Loss: 2.9911e-01 (n=40000)), Acc@1:  89.55 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 341.99it/s, avg_metrics=Loss: 4.9694e-01 (n=10000)), Acc@1:  84.14 (n=10000))]\n",
      "Training Epoch 43: 100%|██████████| 625/625 [00:08<00:00, 75.40it/s, avg_metrics=Loss: 2.8791e-01 (n=40000)), Acc@1:  89.96 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.19it/s, avg_metrics=Loss: 5.0691e-01 (n=10000)), Acc@1:  84.19 (n=10000))]\n",
      "Training Epoch 44: 100%|██████████| 625/625 [00:08<00:00, 75.04it/s, avg_metrics=Loss: 2.8757e-01 (n=40000)), Acc@1:  89.99 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.19it/s, avg_metrics=Loss: 5.0069e-01 (n=10000)), Acc@1:  84.36 (n=10000))]\n",
      "Training Epoch 45: 100%|██████████| 625/625 [00:08<00:00, 74.22it/s, avg_metrics=Loss: 2.7278e-01 (n=40000)), Acc@1:  90.62 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.79it/s, avg_metrics=Loss: 5.2166e-01 (n=10000)), Acc@1:  84.03 (n=10000))]\n",
      "Training Epoch 46: 100%|██████████| 625/625 [00:08<00:00, 75.00it/s, avg_metrics=Loss: 2.6173e-01 (n=40000)), Acc@1:  90.85 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 343.87it/s, avg_metrics=Loss: 5.4703e-01 (n=10000)), Acc@1:  83.62 (n=10000))]\n",
      "Training Epoch 47: 100%|██████████| 625/625 [00:08<00:00, 74.41it/s, avg_metrics=Loss: 2.5728e-01 (n=40000)), Acc@1:  91.13 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 346.29it/s, avg_metrics=Loss: 4.6645e-01 (n=10000)), Acc@1:  85.60 (n=10000))]\n",
      "Training Epoch 48: 100%|██████████| 625/625 [00:08<00:00, 75.45it/s, avg_metrics=Loss: 2.4952e-01 (n=40000)), Acc@1:  91.19 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 335.96it/s, avg_metrics=Loss: 5.2630e-01 (n=10000)), Acc@1:  84.73 (n=10000))]\n",
      "Training Epoch 49: 100%|██████████| 625/625 [00:08<00:00, 74.96it/s, avg_metrics=Loss: 2.4542e-01 (n=40000)), Acc@1:  91.44 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 353.68it/s, avg_metrics=Loss: 4.8326e-01 (n=10000)), Acc@1:  85.23 (n=10000))]\n",
      "Training Epoch 50: 100%|██████████| 625/625 [00:08<00:00, 74.42it/s, avg_metrics=Loss: 2.3422e-01 (n=40000)), Acc@1:  92.01 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 344.94it/s, avg_metrics=Loss: 4.9634e-01 (n=10000)), Acc@1:  85.08 (n=10000))]\n",
      "Training Epoch 51: 100%|██████████| 625/625 [00:08<00:00, 75.02it/s, avg_metrics=Loss: 1.6293e-01 (n=40000)), Acc@1:  94.42 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.46it/s, avg_metrics=Loss: 4.7331e-01 (n=10000)), Acc@1:  86.44 (n=10000))]\n",
      "Training Epoch 52: 100%|██████████| 625/625 [00:08<00:00, 75.39it/s, avg_metrics=Loss: 1.4739e-01 (n=40000)), Acc@1:  95.04 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 346.36it/s, avg_metrics=Loss: 4.7099e-01 (n=10000)), Acc@1:  86.50 (n=10000))]\n",
      "Training Epoch 53: 100%|██████████| 625/625 [00:08<00:00, 74.50it/s, avg_metrics=Loss: 1.4329e-01 (n=40000)), Acc@1:  95.09 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 351.86it/s, avg_metrics=Loss: 4.7813e-01 (n=10000)), Acc@1:  86.76 (n=10000))]\n",
      "Training Epoch 54: 100%|██████████| 625/625 [00:08<00:00, 75.43it/s, avg_metrics=Loss: 1.3314e-01 (n=40000)), Acc@1:  95.54 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 345.17it/s, avg_metrics=Loss: 4.8886e-01 (n=10000)), Acc@1:  86.63 (n=10000))]\n",
      "Training Epoch 55: 100%|██████████| 625/625 [00:08<00:00, 74.36it/s, avg_metrics=Loss: 1.3032e-01 (n=40000)), Acc@1:  95.61 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 346.15it/s, avg_metrics=Loss: 4.8748e-01 (n=10000)), Acc@1:  86.91 (n=10000))]\n",
      "Training Epoch 56: 100%|██████████| 625/625 [00:08<00:00, 75.41it/s, avg_metrics=Loss: 1.2868e-01 (n=40000)), Acc@1:  95.61 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.20it/s, avg_metrics=Loss: 4.8815e-01 (n=10000)), Acc@1:  86.77 (n=10000))]\n",
      "Training Epoch 57: 100%|██████████| 625/625 [00:08<00:00, 75.25it/s, avg_metrics=Loss: 1.2561e-01 (n=40000)), Acc@1:  95.63 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.45it/s, avg_metrics=Loss: 4.9551e-01 (n=10000)), Acc@1:  86.83 (n=10000))]\n",
      "Training Epoch 58: 100%|██████████| 625/625 [00:08<00:00, 74.64it/s, avg_metrics=Loss: 1.2073e-01 (n=40000)), Acc@1:  95.94 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.58it/s, avg_metrics=Loss: 4.9930e-01 (n=10000)), Acc@1:  86.74 (n=10000))]\n",
      "Training Epoch 59: 100%|██████████| 625/625 [00:08<00:00, 75.15it/s, avg_metrics=Loss: 1.2158e-01 (n=40000)), Acc@1:  95.88 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 342.52it/s, avg_metrics=Loss: 4.9231e-01 (n=10000)), Acc@1:  86.81 (n=10000))]\n",
      "Training Epoch 60: 100%|██████████| 625/625 [00:08<00:00, 75.44it/s, avg_metrics=Loss: 1.1224e-01 (n=40000)), Acc@1:  96.07 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.23it/s, avg_metrics=Loss: 5.0169e-01 (n=10000)), Acc@1:  86.51 (n=10000))]\n",
      "Training Epoch 61: 100%|██████████| 625/625 [00:08<00:00, 74.30it/s, avg_metrics=Loss: 1.1496e-01 (n=40000)), Acc@1:  96.11 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 346.23it/s, avg_metrics=Loss: 5.0610e-01 (n=10000)), Acc@1:  86.74 (n=10000))]\n",
      "Training Epoch 62: 100%|██████████| 625/625 [00:08<00:00, 75.49it/s, avg_metrics=Loss: 1.1266e-01 (n=40000)), Acc@1:  96.08 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 345.07it/s, avg_metrics=Loss: 5.0378e-01 (n=10000)), Acc@1:  86.77 (n=10000))]\n",
      "Training Epoch 63: 100%|██████████| 625/625 [00:08<00:00, 74.28it/s, avg_metrics=Loss: 1.1031e-01 (n=40000)), Acc@1:  96.28 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 344.80it/s, avg_metrics=Loss: 5.0954e-01 (n=10000)), Acc@1:  86.87 (n=10000))]\n",
      "Training Epoch 64: 100%|██████████| 625/625 [00:08<00:00, 75.13it/s, avg_metrics=Loss: 1.1061e-01 (n=40000)), Acc@1:  96.27 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 346.16it/s, avg_metrics=Loss: 5.1490e-01 (n=10000)), Acc@1:  86.82 (n=10000))]\n",
      "Training Epoch 65: 100%|██████████| 625/625 [00:08<00:00, 75.30it/s, avg_metrics=Loss: 1.0770e-01 (n=40000)), Acc@1:  96.32 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 341.98it/s, avg_metrics=Loss: 5.1220e-01 (n=10000)), Acc@1:  86.78 (n=10000))]\n",
      "Training Epoch 66: 100%|██████████| 625/625 [00:08<00:00, 74.50it/s, avg_metrics=Loss: 1.0226e-01 (n=40000)), Acc@1:  96.47 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.45it/s, avg_metrics=Loss: 5.1735e-01 (n=10000)), Acc@1:  86.79 (n=10000))]\n",
      "Training Epoch 67: 100%|██████████| 625/625 [00:08<00:00, 75.05it/s, avg_metrics=Loss: 1.0596e-01 (n=40000)), Acc@1:  96.35 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 342.23it/s, avg_metrics=Loss: 5.2072e-01 (n=10000)), Acc@1:  87.00 (n=10000))]\n",
      "Training Epoch 68: 100%|██████████| 625/625 [00:08<00:00, 75.14it/s, avg_metrics=Loss: 9.9734e-02 (n=40000)), Acc@1:  96.58 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.08it/s, avg_metrics=Loss: 5.2377e-01 (n=10000)), Acc@1:  86.83 (n=10000))]\n",
      "Training Epoch 69: 100%|██████████| 625/625 [00:08<00:00, 74.43it/s, avg_metrics=Loss: 9.9320e-02 (n=40000)), Acc@1:  96.63 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.38it/s, avg_metrics=Loss: 5.2503e-01 (n=10000)), Acc@1:  86.73 (n=10000))]\n",
      "Training Epoch 70: 100%|██████████| 625/625 [00:08<00:00, 75.41it/s, avg_metrics=Loss: 9.7506e-02 (n=40000)), Acc@1:  96.69 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 347.23it/s, avg_metrics=Loss: 5.3960e-01 (n=10000)), Acc@1:  86.74 (n=10000))]\n",
      "Training Epoch 71: 100%|██████████| 625/625 [00:08<00:00, 74.40it/s, avg_metrics=Loss: 9.7007e-02 (n=40000)), Acc@1:  96.65 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 346.67it/s, avg_metrics=Loss: 5.2745e-01 (n=10000)), Acc@1:  86.80 (n=10000))]\n",
      "Training Epoch 72: 100%|██████████| 625/625 [00:08<00:00, 75.54it/s, avg_metrics=Loss: 9.4745e-02 (n=40000)), Acc@1:  96.79 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 348.72it/s, avg_metrics=Loss: 5.3274e-01 (n=10000)), Acc@1:  86.71 (n=10000))]\n",
      "Training Epoch 73: 100%|██████████| 625/625 [00:08<00:00, 75.38it/s, avg_metrics=Loss: 9.3678e-02 (n=40000)), Acc@1:  96.78 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.18it/s, avg_metrics=Loss: 5.5299e-01 (n=10000)), Acc@1:  86.77 (n=10000))]\n",
      "Training Epoch 74: 100%|██████████| 625/625 [00:08<00:00, 74.17it/s, avg_metrics=Loss: 9.1958e-02 (n=40000)), Acc@1:  96.86 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 345.57it/s, avg_metrics=Loss: 5.3942e-01 (n=10000)), Acc@1:  86.83 (n=10000))]\n",
      "Training Epoch 75: 100%|██████████| 625/625 [00:08<00:00, 75.01it/s, avg_metrics=Loss: 8.8139e-02 (n=40000)), Acc@1:  97.01 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 353.65it/s, avg_metrics=Loss: 5.5249e-01 (n=10000)), Acc@1:  87.01 (n=10000))]\n",
      "Training Epoch 76: 100%|██████████| 625/625 [00:08<00:00, 74.36it/s, avg_metrics=Loss: 9.0254e-02 (n=40000)), Acc@1:  96.95 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 338.67it/s, avg_metrics=Loss: 5.3835e-01 (n=10000)), Acc@1:  86.94 (n=10000))]\n",
      "Training Epoch 77: 100%|██████████| 625/625 [00:08<00:00, 75.33it/s, avg_metrics=Loss: 8.8501e-02 (n=40000)), Acc@1:  96.96 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 352.55it/s, avg_metrics=Loss: 5.5192e-01 (n=10000)), Acc@1:  86.94 (n=10000))]\n",
      "Training Epoch 78: 100%|██████████| 625/625 [00:08<00:00, 75.00it/s, avg_metrics=Loss: 8.7236e-02 (n=40000)), Acc@1:  96.96 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 345.57it/s, avg_metrics=Loss: 5.5420e-01 (n=10000)), Acc@1:  86.79 (n=10000))]\n",
      "Training Epoch 79: 100%|██████████| 625/625 [00:08<00:00, 74.16it/s, avg_metrics=Loss: 8.4037e-02 (n=40000)), Acc@1:  97.12 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.79it/s, avg_metrics=Loss: 5.6135e-01 (n=10000)), Acc@1:  86.85 (n=10000))]\n",
      "Training Epoch 80: 100%|██████████| 625/625 [00:08<00:00, 75.18it/s, avg_metrics=Loss: 8.4233e-02 (n=40000)), Acc@1:  97.06 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 341.99it/s, avg_metrics=Loss: 5.6202e-01 (n=10000)), Acc@1:  86.83 (n=10000))]\n",
      "Training Epoch 81: 100%|██████████| 625/625 [00:08<00:00, 74.40it/s, avg_metrics=Loss: 8.2603e-02 (n=40000)), Acc@1:  97.21 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.95it/s, avg_metrics=Loss: 5.6064e-01 (n=10000)), Acc@1:  86.86 (n=10000))]\n",
      "Training Epoch 82: 100%|██████████| 625/625 [00:08<00:00, 75.23it/s, avg_metrics=Loss: 7.9323e-02 (n=40000)), Acc@1:  97.20 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 353.09it/s, avg_metrics=Loss: 5.6875e-01 (n=10000)), Acc@1:  86.73 (n=10000))]\n",
      "Training Epoch 83: 100%|██████████| 625/625 [00:08<00:00, 74.72it/s, avg_metrics=Loss: 8.3151e-02 (n=40000)), Acc@1:  97.14 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 344.05it/s, avg_metrics=Loss: 5.6419e-01 (n=10000)), Acc@1:  86.68 (n=10000))]\n",
      "Training Epoch 84: 100%|██████████| 625/625 [00:08<00:00, 74.86it/s, avg_metrics=Loss: 8.3623e-02 (n=40000)), Acc@1:  97.13 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 350.89it/s, avg_metrics=Loss: 5.5681e-01 (n=10000)), Acc@1:  86.69 (n=10000))]\n",
      "Training Epoch 85: 100%|██████████| 625/625 [00:08<00:00, 75.14it/s, avg_metrics=Loss: 7.7750e-02 (n=40000)), Acc@1:  97.35 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.06it/s, avg_metrics=Loss: 5.7039e-01 (n=10000)), Acc@1:  86.66 (n=10000))]\n",
      "Training Epoch 86: 100%|██████████| 625/625 [00:08<00:00, 74.57it/s, avg_metrics=Loss: 7.3814e-02 (n=40000)), Acc@1:  97.49 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 346.92it/s, avg_metrics=Loss: 5.7497e-01 (n=10000)), Acc@1:  86.69 (n=10000))]\n",
      "Training Epoch 87: 100%|██████████| 625/625 [00:08<00:00, 75.21it/s, avg_metrics=Loss: 7.6082e-02 (n=40000)), Acc@1:  97.33 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 342.30it/s, avg_metrics=Loss: 5.7292e-01 (n=10000)), Acc@1:  86.68 (n=10000))]\n",
      "Training Epoch 88: 100%|██████████| 625/625 [00:08<00:00, 74.13it/s, avg_metrics=Loss: 7.4413e-02 (n=40000)), Acc@1:  97.49 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 348.35it/s, avg_metrics=Loss: 5.8151e-01 (n=10000)), Acc@1:  86.81 (n=10000))]\n",
      "Training Epoch 89: 100%|██████████| 625/625 [00:08<00:00, 75.29it/s, avg_metrics=Loss: 7.5786e-02 (n=40000)), Acc@1:  97.38 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 345.69it/s, avg_metrics=Loss: 5.7904e-01 (n=10000)), Acc@1:  87.00 (n=10000))]\n",
      "Training Epoch 90: 100%|██████████| 625/625 [00:08<00:00, 74.89it/s, avg_metrics=Loss: 7.1396e-02 (n=40000)), Acc@1:  97.54 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 343.16it/s, avg_metrics=Loss: 5.8541e-01 (n=10000)), Acc@1:  86.74 (n=10000))]\n",
      "Training Epoch 91: 100%|██████████| 625/625 [00:08<00:00, 74.24it/s, avg_metrics=Loss: 7.0083e-02 (n=40000)), Acc@1:  97.56 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 346.60it/s, avg_metrics=Loss: 5.8954e-01 (n=10000)), Acc@1:  87.04 (n=10000))]\n",
      "Training Epoch 92: 100%|██████████| 625/625 [00:08<00:00, 75.27it/s, avg_metrics=Loss: 6.8409e-02 (n=40000)), Acc@1:  97.59 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 344.34it/s, avg_metrics=Loss: 5.8991e-01 (n=10000)), Acc@1:  86.99 (n=10000))]\n",
      "Training Epoch 93: 100%|██████████| 625/625 [00:08<00:00, 74.31it/s, avg_metrics=Loss: 6.8657e-02 (n=40000)), Acc@1:  97.60 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 352.02it/s, avg_metrics=Loss: 6.0145e-01 (n=10000)), Acc@1:  86.81 (n=10000))]\n",
      "Training Epoch 94: 100%|██████████| 625/625 [00:08<00:00, 75.12it/s, avg_metrics=Loss: 7.0221e-02 (n=40000)), Acc@1:  97.63 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 346.76it/s, avg_metrics=Loss: 6.0095e-01 (n=10000)), Acc@1:  86.84 (n=10000))]\n",
      "Training Epoch 95: 100%|██████████| 625/625 [00:08<00:00, 75.42it/s, avg_metrics=Loss: 6.7644e-02 (n=40000)), Acc@1:  97.61 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 351.59it/s, avg_metrics=Loss: 5.9150e-01 (n=10000)), Acc@1:  86.96 (n=10000))]\n",
      "Training Epoch 96: 100%|██████████| 625/625 [00:08<00:00, 74.66it/s, avg_metrics=Loss: 6.5865e-02 (n=40000)), Acc@1:  97.76 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 351.89it/s, avg_metrics=Loss: 6.1808e-01 (n=10000)), Acc@1:  86.32 (n=10000))]\n",
      "Training Epoch 97: 100%|██████████| 625/625 [00:08<00:00, 74.89it/s, avg_metrics=Loss: 6.5709e-02 (n=40000)), Acc@1:  97.76 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 349.91it/s, avg_metrics=Loss: 6.0593e-01 (n=10000)), Acc@1:  86.70 (n=10000))]\n",
      "Training Epoch 98: 100%|██████████| 625/625 [00:08<00:00, 74.54it/s, avg_metrics=Loss: 6.2912e-02 (n=40000)), Acc@1:  97.90 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.004 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 343.51it/s, avg_metrics=Loss: 6.0904e-01 (n=10000)), Acc@1:  86.84 (n=10000))]\n",
      "Training Epoch 99: 100%|██████████| 625/625 [00:08<00:00, 75.15it/s, avg_metrics=Loss: 6.7414e-02 (n=40000)), Acc@1:  97.68 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 333.58it/s, avg_metrics=Loss: 6.1729e-01 (n=10000)), Acc@1:  86.74 (n=10000))]\n",
      "Training Epoch 100: 100%|██████████| 625/625 [00:08<00:00, 75.44it/s, avg_metrics=Loss: 6.3583e-02 (n=40000)), Acc@1:  97.82 (n=40000)), Data_Time:  0.000 (n=625)), Batch_Time:  0.003 (n=625))]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 344.69it/s, avg_metrics=Loss: 6.1040e-01 (n=10000)), Acc@1:  86.89 (n=10000))]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy@1</td><td>▁▃▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>Train Loss</td><td>█▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy@1</td><td>▁▃▅▅▆▆▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>Validation Loss</td><td>█▆▄▃▂▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy@1</td><td>97.8175</td></tr><tr><td>Train Loss</td><td>0.06358</td></tr><tr><td>Validation Accuracy@1</td><td>86.89</td></tr><tr><td>Validation Loss</td><td>0.6104</td></tr><tr><td>epoch</td><td>99</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fanciful-feather-20</strong> at: <a href='https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning/runs/5lkoybmk' target=\"_blank\">https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning/runs/5lkoybmk</a><br/> View project at: <a href='https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241010_080337-5lkoybmk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'data_root_dir': '/datasets',\n",
    "    'batch_size': 64, # 수정 x\n",
    "    'learning_rate': 1e-3,\n",
    "    'num_epochs': 100,\n",
    "    'model_name': 'vgg16',\n",
    "    \"dataset\": \"CIFAR10\",\n",
    "    'wandb_project_name': 'CIFAR10_hyperparameter_tuning',\n",
    "\n",
    "    \"checkpoint_save_interval\" : 20,\n",
    "    \"checkpoint_path\" : \"checkpoints/checkpoint.pth\",\n",
    "    \"best_model_path\" : \"checkpoints/best_model.pth\",\n",
    "    \"load_from_checkpoint\" : None,    # Options: \"latest\", \"best\", or None\n",
    "}\n",
    "train_main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실험이 모두 끝나면 best model에 대해 test set성능을 평가한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c23bcd9b4b47bc9247c90b187e2922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111305501156797, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/deep_learning_lab/lab_05/wandb/run-20241010_082014-x8ck3k6v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning/runs/x8ck3k6v' target=\"_blank\">volcanic-paper-21</a></strong> to <a href='https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning/runs/x8ck3k6v' target=\"_blank\">https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning/runs/x8ck3k6v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using model vgg16 with 134301514 parameters (134301514 trainable)\n",
      "=> loaded checkpoint 'checkpoints/best_model.pth' (epoch 100)\n",
      "Running test evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation/Test: 100%|██████████| 157/157 [00:00<00:00, 356.92it/s, avg_metrics=Loss: 6.1863e-01 (n=10000)), Acc@1:  86.56 (n=10000))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.55999755859375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy@1</td><td>▁</td></tr><tr><td>Test Loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy@1</td><td>86.56</td></tr><tr><td>Test Loss</td><td>0.61863</td></tr><tr><td>epoch</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-paper-21</strong> at: <a href='https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning/runs/x8ck3k6v' target=\"_blank\">https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning/runs/x8ck3k6v</a><br/> View project at: <a href='https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/sangmin881717-university-of-suwon/CIFAR10_hyperparameter_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241010_082014-x8ck3k6v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_testmode = {\n",
    "    **config, \n",
    "    'test_mode': True, # True if evaluating only test set\n",
    "    'load_from_checkpoint': 'best'\n",
    "}\n",
    "\n",
    "train_main(config_testmode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>제출물</mark>\n",
    "\n",
    "1. 본인 이름이 나오도록 wandb 결과 화면을 캡처하여 `YOUR_PRIVATE_REPOSITORY_NAME/lab_05/wandb_results.png`에 저장한다. (5 points)\n",
    "2. 결과를 table로 정리한 뒤 그 아래에 분석 및 논의를 작성 한다. (15 points)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### wandb 결과\n",
    "\n",
    "<center><img src=\"./wandb_results.png\" width=\"1000px\"></img></center>\n",
    "\n",
    "#### 5개 이상의 실험 결과\n",
    "\n",
    "| 모델 | 실험 조건 | val_accuracy | 설명  |\n",
    "|------|----------|--------------|------|\n",
    "| resnet50   | optimizer: SGD, lr: 1e-3, epochs: 100     |      50.5      |   Default SGD만 적용한 기본 모델   |\n",
    "|  resnet50   | optimizer:SGD(momentum=0.9 , weight decay), lr: 1e-3, epochs: 100          |        58.79      |  SGD에 모멘텀을 추가    |\n",
    "|   vgg16   |    optimizer:SGD(momentum=0.9, weight decay), lr: 1e-3, data proccessing, epochs: 100      |   86.4           |    resnet에서 vgg16으로 모델 변경, 데이터 전처리 적용  |\n",
    "|  vgg16    |    optimizer:SGD(momentum=0.9, weight decay), lr: 1e-3, data proccessing, Leaky ReLU, epochs: 100     | 86.5             |   Leaky ReLU 적용  |\n",
    "|  vgg16    |    optimizer:SGD(momentum=0.9, weight decay), lr: 1e-3, data proccessing, actication, Weight initialization, epochs: 100       |       86.89      |   가중치 초기화 적용   |\n",
    "\n",
    "best model test_set accuracy: \n",
    "\n",
    "#### 분석 및 논의\n",
    "optimizer의 변화가 아닌 Data Proccessing, 활성화 함수 변경 등을 통해 validation accuracy를 높이고 싶어서 모든 optimizer는 SGD로 통일했습니다.\n",
    "처음에는 아무것도 적용하지 않은 SGD로 테스트를 했고 50%라는 낮은 정확도를 기록했습니다.\n",
    "정확도를 높이기 위해 SGD에 momentum을 0.9로 적용해 테스트를 했고 약 59% 정도를 기록했지만, 기대한 바와 다르게 정확도는 8% 정도만 상승했습니다.\n",
    "방향성을 다르게 해서 모델을 resnet50에서 vgg16으로 변경하고, 데이터 전처리를 진행해 데이터의 다양성을 높여서 테스트를 진행한 결과 정확도가 86.4%로 크게 상승한 것을 확인했습니다. 데이터 전처리 후 정확도가 크게 증가하는 것으로 보아 데이터의 다양성이 필요했던 것이 아닐까 생각했습니다.\n",
    "이후 정확도를 더욱 높이기 위해 활성화 함수를 Leaky ReLU로 변경, 가중치 초기화 등을 적용했지만 정확도는 3번째 테스트와 유사하게 나왔습니다.\n",
    "\n",
    "결론적으로 정확도 상승에 큰 기여를 한 것은 데이터 전처리가 아닐까 생각됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### Lab을 마무리 짓기 전 저장된 checkpoint를 모두 지워 저장공간을 확보한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "if os.path.exists('checkpoints/'):\n",
    "    shutil.rmtree('checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
